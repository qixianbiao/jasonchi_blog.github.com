<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Pairwise Rotation Invariant Co-occurrence Local Binary Pattern</title>
</head>
<body>
<div align="center">
<div style="width:950px; text-align:left;">


  <center>
    <p><b>
    <font size=5>
    Pairwise Rotation Invariant Co-occurrence Local Binary Pattern
    </font>
    </b> 
    </p>
  </center>
  
  
  <center>
    <p><a href="https://sites.google.com/site/xianbiaoqi/"><font color="blue">Xianbiao Qi</font></a>, 
      
      
      <a href="http://research.microsoft.com/en-us/people/rxiao/"><font color="black">Rong Xiao</font></a>, 
      
      
      
      <a href="http://sourcedb.siat.cas.cn/zw/zjrc/fyjy/201005/t20100513_2844772.html"> <font color="green">Yu Qiao</font></a>,  
      
      
      
      <a href="http://www.pris.net.cn/teacher/guojun"><font color="blue"> Jun Guo</font> </a>, 
      
     
      <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml"> <font color="red">Xiaoou Tang</font></a>
      
      
    </p>
    <p><font color="blue">Beijing University of Posts and Telecommunications</font>, <font color="black">Microsoft Coroperation</font>, </p>
    <p><font color="green">Shenzhen Institutes of Advanced Technology of Chinese Academy of Sciences</font>, <font color="red">The Chinese University of Hong Kong</font>.</p>
  </center>
<h2>
  <b>
  Abstract
  </b> 
  </h2> 
 <p>Designing simple and effective feature is a fundamental problem in computer vision. However, it is usually difficult to achieve great trade-off between the discriminative power and transformation invriance. Spatial co-occurrence could boost the discriminative power of the features, but it also suffer from the geometric and photometric variations. In this work, we investigate rotation invariant property of co-occurrence feature, and introduce a novel pairwise rotation invariant co-occurrence local binary pattern (PRI-CoLBP) feature which incorporates two types of context, spatial co-occurrence and orientation co-occurrence. Different from traditional rotation invariant local features, pairwise rotation invariant co-occurrence features preserve the relative angles between the orientations of individual features. The relative angle depicts the local curvature information, which is discriminative.</p>
 <p>The proposed PRI-CoLBP is computationally efficient and has been applied to six types of applications, including texture classification, material classification, flower recognition, leaf recognition, food recognition and scene recognition. Superior performance on such applications demonstrate the effectiveness of the proposed PRI-CoLBP.</p>
 <p>&nbsp;</p>
 
 <h2><b>
  Paper
  </b>
  </h2>
  Xianbiao Qi, Rong Xiao, Yu Qiao, Jun Guo, Xiaoou Tang. <font color="#669933"><i>Pairwise Rotation Invariant Co-occurrence Local Binary Pattern</i></font>. Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).
As an extended version of ECCV 2012 paper <a href="http://link.springer.com/content/pdf/10.1007%2F978-3-642-33783-3_12">pdf</a> [Qi and Xiao et al.]. 
 <p>&nbsp;</p>  

<h2>Features</h2>
<p>Disciminative power and transformation invariance are the two most important properties of local features. Spatial co-occurrence could greatly boost the discriminative power of the features, but its transformation invariant is hard to obtain. To achieve transformation invariance for co-occurrence feature, we should promise the following two conditions:</p>
<ol>
<li>The correspondence of described points set should be promised under different image transformations.</li>
<li>The descriptor for points set is transformation invariant.</li>
</ol> 
<p>In practise, we densely sample each points of the images. For the same point <b>A</b> in same scene under different image transformation, we should promise that the same points set {<b>B<sub>i</sub></b>} could be uniquely determined.</p>

When the correspondence of described points is promised, we should promise that the descriptor for the same points set
 is transformation invariant.


 <p>&nbsp;</p>

<h2>Propoerties</h2>
<p>The proposed feature has the following properties:</p>

<ul>
<li>Gray-scale invariant. </li>
<li>Rotation invariant.</li>
<li>Extremely effecitive on describing texture information.</li>
<li>Computationally efficient.</li>
<li>Effective on capturing edges and contours structures through paying more attention to such regions.</li>
<li>Flexible to fuse color information. </li>
</ul> 
<p>&nbsp;</p>


<h2>Source Code</h2>
<p><font color="#66FF00"><a href="http://dl.dropbox.com/u/66608698/SourceCode/PRICoLBP.zip">Source Code Download</a></font>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
<font color="#66FF00"><a href="http://pan.baidu.com/share/link?shareid=247373&uk=237669910">Source Code Download (Access Link From China)</a></font>
</p>



<p>With the provided source code, you could easily run and get our results. </p>
<p>&nbsp;</p>






<h2>Applications</h2>
The proposed feature is extremely effective on a lot of applications, such as texture classification, material recognition, flower recognition, leaf recognition, food recognition and scene classification.

<p>In our work, we test our feature on eight datasets, including <a href="http://www.ux.uis.no/~tranden/brodatz.html">Brodatz</a>, <a href="http://www.cs.columbia.edu/CAVE/software/curet/">CUReT</a>, and <a href="http://www.nada.kth.se/cvap/databases/kth-tips/download.html">KTH-TIPS</a>, <a href="http://people.csail.mit.edu/celiu/CVPR2010/index.html">Flickr Material Database</a>, <a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html">Oxford Flower 102</a>, 
<a href="http://www.isy.liu.se/cvl/ImageDB/public/blad/">Swedish Leaf Database</a>, <a href="http://pfid.intel-research.net/">Food database</a> and <a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/">Scene-15 database</a>.  </p>  The following table presents the detailed information for each database.

<center>
<img src="images/databases.png" width="900" height="158" alt="Databases" />
</center> 

<p>Detailed experimental results and analysis for all applications can be found at follows:
<p> </p>
<center>
<font size=4><a href="Texture.html">Texture Classification</a></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

<font size=4><a href="Material.html">Material Recognition</a></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<font size=4><a href="Flower.html">Flower Recognition</a></font>
</center>
<p> </p>

<div style="text-align:center">  <font size=4><a href="Leaf.html">Leaf Recognition</a></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<font size=4><a href="Food.html">Food Recognition</a></font>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<font size=4><a href="Scene.html">Scene Classification</a></font>  </div>
<p>&nbsp;</p>  



<h2>Feature Matrices</h2>
<p>The feature matrices are avialble at <a href="features/features.html">This Link</a>.</p>
<p>&nbsp;</p>

  
<h2>Relevant Toolboxs</h2>
<p><a href="http://www.vlfeat.org/">VLFeat</a>.</p>
<p><a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">LibSVM</a>.</p>
<p>&nbsp;</p>


  
</p>
<h2>Acknowledgments</h2>
<p>This work was supported by National Natural Science Foundation of China(Grant No.61005004 and 61175011),the 111 project(Grant No.B08004), and the Fundamental Research Funds for the Central Universities(Grant No.2012RC0108).</p>
<p>&nbsp;</p>
<p>&nbsp;</p>


</div>
</div>
</body>
</html>
